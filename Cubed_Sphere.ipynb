{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030d9a9a",
   "metadata": {},
   "source": [
    "### Retrieve data\n",
    "\n",
    "Define the variables and levels we want to retrieve. Single-level variables ignore the \"levels\" parameter. Also note that not all variables in the ERA5 dataset are coded with their parameter names as of now. We also take a reduced sample of years in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cba0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['geopotential','2m_temperature']\n",
    "levels = [500]\n",
    "years = list(range(1979, 2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.pardir)\n",
    "from DLWP.data import ERA5Reanalysis\n",
    "\n",
    "data_directory = '/lcrc/project/AIEADA-2/cubed_sphere_implementation/Data'\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "era = ERA5Reanalysis(root_directory=data_directory, file_id='tutorial')\n",
    "era.set_variables(variables)\n",
    "era.set_levels(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c6ed9",
   "metadata": {},
   "source": [
    "Download data! Automatically uses multi-processing to retrieve multiple files at a time. Note the parameter `hourly` says we're retrieving only every 3rd hour in the data, which is available hourly. The optional parameter passed to the retrieval package specifies that we want data interpolated to a 2-by-2 latitude-longitude grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685db40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.retrieve(variables, levels, years=years, hourly=3,\n",
    "             request_kwargs={'grid': [2., 2.]}, verbose=True, delete_temporary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "era.open()\n",
    "print(era.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87f317",
   "metadata": {},
   "source": [
    "Now we use the DLWP.model.Preprocessor tool to generate a new data file ready for use in a DLWP Keras model. Some preliminaries... Note that we assign level \"0\" to the single-level 2m temperature data. I highly recommend using \"pairwise\" data processing, which means that each variable is matched to a level pair-wise. The length of the variables and levels lists should be the same. Also note that you only need to specify whole days in the dates. It takes care of the hourly data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d294a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from DLWP.data.era5 import get_short_name\n",
    "\n",
    "dates = list(pd.date_range('1979-01-01', '2018-12-31', freq='D').to_pydatetime())\n",
    "variables = get_short_name(variables)\n",
    "levels = [500,0]\n",
    "processed_file = '%s/tutorial_z500_t2m_no_scale.nc' % data_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2dc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import Preprocessor\n",
    "\n",
    "pp = Preprocessor(era, predictor_file=processed_file)\n",
    "pp.data_to_series(batch_samples=10000, variables=variables, levels=levels, pairwise=True,\n",
    "                  scale_variables=True, overwrite=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pp.data)\n",
    "pp.data.drop('varlev').to_netcdf(processed_file + '.nocoord')\n",
    "era.close()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfe0ce",
   "metadata": {},
   "source": [
    "## Remapping training data to the cubed sphere\n",
    "\n",
    "The novel addition in DLWP-CS is the ability to train convolutional neural networks on data mapped to the cubed sphere. The re-mapping is performed offline from the model training/inference. \n",
    "\n",
    "#### Required packages\n",
    "\n",
    "We use the TempestRemap library for cubed sphere remapping which is available as a pre-compiled conda package. Let's start by installing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge tempest-remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e582117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.pardir)\n",
    "from DLWP.remap import CubeSphereRemap\n",
    "\n",
    "data_directory = '/lcrc/project/AIEADA-2/cubed_sphere_implementation/Data'\n",
    "processed_file = '%s/tutorial_z500_t2m.nc' % data_directory\n",
    "remapped_file = '%s/tutorial_z500_t2m_CS.nc' % data_directory\n",
    "\n",
    "csr = CubeSphereRemap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67354044",
   "metadata": {},
   "outputs": [],
   "source": [
    "csr.generate_offline_maps(lat=91, lon=180, res=48, inverse_lat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7f7cb",
   "metadata": {},
   "source": [
    "Apply the forward map, saving to a temporary file. We specify to operate on the variable `predictors`, which is the only variable in the processed data. TempestRemap is very finicky about metadata in netCDF files, sometimes failing with segmentation faults for no apparent reason. I've found that the most common crash is because it does not like the string coordinate values in the `'varlev'` coordinate. If you used the command in the previous tutorial to produce an extra \"nocoord\" version of this file, you might *have to* use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeafb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csr.remap(processed_file + '.nocoord', '%s/temp.nc' % data_directory, '--var', 'predictors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c219e7",
   "metadata": {},
   "source": [
    "By default, TempestRemap has a 1-dimensional spatial coordinate. We convert the file to 3-dimensional faces (face, height, width). A few other points here:  \n",
    "- Even if TempestRemap does not crash, it will probably delete the string coordinates, and sometimes the sample time coordinate as well, so it's a good idea to use this feature.  \n",
    "- We also take advantage of the `chunking` parameter to save data with ideal chunking when using the file for training and evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "csr.convert_to_faces('%s/temp.nc' % data_directory, \n",
    "                     remapped_file,\n",
    "                     coord_file=processed_file,\n",
    "                     chunking={'sample': 1, 'varlev': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5165f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('%s/temp.nc' % data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409938fc",
   "metadata": {},
   "source": [
    "## Training a DLWP-CS model\n",
    "\n",
    "Now we use the data processed in the previous two notebooks to train a convolutional neural network on weather data mapped to the cubed sphere. We will construct the same convolutional neural network with the cubed sphere as in *Weyn et al. (2020)*, with the exception of having only two variables (Z500 and T2) instead of their four, and without the constant land-sea mask and topography data. This will seem like a fairly involved example but much simpler constructions are also possible using the `DLWPNeuralNet` class instead of the functional Keras API. I also highly recommend having this model train on a GPU with at least 4 GB of video memory.\n",
    "\n",
    "#### Required packages\n",
    "\n",
    "No new packages are needed here beyond the main DLWP-CS requirements in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da336583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.pardir)\n",
    "\n",
    "root_directory = '/lcrc/project/AIEADA-2/cubed_sphere_implementation/'\n",
    "predictor_file = os.path.join(root_directory, 'Data', 'tutorial_z500_t2m_CS.nc')\n",
    "model_file = os.path.join(root_directory, 'dlwp-cs_tutorial')\n",
    "log_directory = os.path.join(root_directory, 'logs', 'dlwp-cs_tutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb341996",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_name = 'unet2'\n",
    "base_filter_number = 32\n",
    "min_epochs = 0\n",
    "max_epochs = 20\n",
    "patience = 2\n",
    "batch_size = 32\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84418e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_selection = {'varlev':['z/500','t2m/0']}\n",
    "add_solar = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18200254",
   "metadata": {},
   "source": [
    "These parameters govern the time stepping in the model.  \n",
    "- `io_time_steps`: the number of input/output time steps directly ingested/predicted by the model  \n",
    "- `integration_steps`: the number of forward sequence steps on which to minimize the loss function of the model  \n",
    "- `data_interval`: the number of steps in the data file that constitute a \"time step.\" Here we use 2, and the data contains data every 3 hours, so the effective time step is 6 h.\n",
    "- `loss_by_step`: either None (equal weighting) or a list of weighting factors for the loss function at each integration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ef45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_time_steps = 2\n",
    "integration_steps = 2\n",
    "data_interval = 2\n",
    "loss_by_step = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = list(pd.date_range('1979-01-01', '2014-12-31 21:00', freq='3H'))\n",
    "validation_set = list(pd.date_range('2015-01-01', '2016-12-31 21:00', freq='3H'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mp_optimizer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8f24f",
   "metadata": {},
   "source": [
    "### Create a DLWP model\n",
    "\n",
    "Since the data generators depend on the model (granted it's an outdated dependency), we make the model instance first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94635cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import DLWPFunctional\n",
    "\n",
    "dlwp = DLWPFunctional(is_convolutional=True, time_dim=io_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee5d71",
   "metadata": {},
   "source": [
    "### Load data and create data generators\n",
    "\n",
    "DLWP-CS includes powerful data generators that produce batches of training data on-the-fly. This enables them to load only the time series into memory instead of repetitive samples of data. On the downside, it makes reading training data from disk virtually impossibly slow. First, load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "data = xr.open_dataset(predictor_file)\n",
    "train_data = data.sel(sample=train_set)\n",
    "validation_data = data.sel(sample=validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438a4a6",
   "metadata": {},
   "source": [
    "Create the training data generator. Here we use the `ArrayDataGenerator` class, which has a nifty pre-processing function to create a single numpy array of data. The `SeriesDataGenerator` is more intuitive and would work equally well. The only reason I don't use the latter is because I thought the overhead when using xarray objects instead of pure numpy might slow things down. It doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model.preprocessing import prepare_data_array\n",
    "from DLWP.model import ArrayDataGenerator\n",
    "\n",
    "print('Loading data to memory...')\n",
    "train_array, input_ind, output_ind, sol = prepare_data_array(train_data, input_sel=io_selection,\n",
    "                                                             output_sel=output, add_insolation=add_solar)\n",
    "generator = ArrayDataGenerator(dlwp, train_array, rank=3, input_slice=input_ind, output_slice=output_ind,\n",
    "                               input_time_steps=io_time_steps, output_time_steps=io_time_steps,\n",
    "                               sequence=integration_steps, interval=data_interval, insolation_array=sol,\n",
    "                               batch_size=batch_size, shuffle=shuffle, channels_last=True,\n",
    "                               drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading validation data to memory...')\n",
    "val_array, input_ind, output_ind, sol = prepare_data_array(validation_data, input_sel=io_selection,\n",
    "                                                           output_sel=output, add_insolation=add_solar)\n",
    "val_generator = ArrayDataGenerator(dlwp, val_array, rank=3, input_slice=input_ind, output_slice=output_ind,\n",
    "                                   input_time_steps=io_time_steps, output_time_steps=io_time_steps,\n",
    "                                   sequence=integration_steps, interval=data_interval, insolation_array=sol,\n",
    "                                   batch_size=batch_size, shuffle=False, channels_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import tf_data_generator\n",
    "\n",
    "input_names = ['main_input'] + ['solar_%d' % i for i in range(1, integration_steps)]\n",
    "tf_train_data = tf_data_generator(generator, batch_size=batch_size, input_names=input_names)\n",
    "tf_val_data = tf_data_generator(val_generator, input_names=input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b799152",
   "metadata": {},
   "source": [
    "### Create the CNN model architecture\n",
    "\n",
    "Now the fun part! Here we create all of the layers that will go into the model. A few notes:  \n",
    "- The generator produces a list of inputs when `integration_steps` is greater than 1:  \n",
    "  - main input, including insolation  \n",
    "  - insolation for step 2  \n",
    "  - insolation for step 3...  \n",
    "- We use our custom layers for padding and convolutions on the cubed sphere  \n",
    "- We can use the Keras 3D layers for operations on the 3D spatial structure of the cubed sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20256411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, UpSampling3D, AveragePooling3D, concatenate, ReLU, Reshape, Concatenate, \\\n",
    "    Permute\n",
    "from DLWP.custom import CubeSpherePadding2D, CubeSphereConv2D\n",
    "\n",
    "# Some shortcut variables. The generator provides the expected shape of the data.\n",
    "cs = generator.convolution_shape\n",
    "cso = generator.output_convolution_shape\n",
    "input_solar = (integration_steps > 1 and add_solar)\n",
    "\n",
    "# Define layers. Must be defined outside of model function so we use the same weights at each integration step.\n",
    "main_input = Input(shape=cs, name='main_input')\n",
    "if input_solar:\n",
    "    solar_inputs = [Input(shape=generator.insolation_shape, name='solar_%d' % d) for d in range(1, integration_steps)]\n",
    "cube_padding_1 = CubeSpherePadding2D(1, data_format='channels_last')\n",
    "pooling_2 = AveragePooling3D((1, 2, 2), data_format='channels_last')\n",
    "up_sampling_2 = UpSampling3D((1, 2, 2), data_format='channels_last')\n",
    "relu = ReLU(negative_slope=0.1, max_value=10.)\n",
    "conv_kwargs = {\n",
    "    'dilation_rate': 1,\n",
    "    'padding': 'valid',\n",
    "    'activation': 'linear',\n",
    "    'data_format': 'channels_last'\n",
    "}\n",
    "skip_connections = 'unet' in cnn_model_name.lower()\n",
    "conv_2d_1 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_1_2 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_1_3 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_2 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_2_2 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_2_3 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_3 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_3_2 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_4 = CubeSphereConv2D(base_filter_number * 4 if skip_connections else base_filter_number * 8, 3, **conv_kwargs)\n",
    "conv_2d_4_2 = CubeSphereConv2D(base_filter_number * 8, 3, **conv_kwargs)\n",
    "conv_2d_5 = CubeSphereConv2D(base_filter_number * 2 if skip_connections else base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_5_2 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_5_3 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_6 = CubeSphereConv2D(base_filter_number if skip_connections else base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_6_2 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_6_3 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_7 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_7_2 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_7_3 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_8 = CubeSphereConv2D(cso[-1], 1, name='output', **conv_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b2a11",
   "metadata": {},
   "source": [
    "Now we actually create the output using the functional API. For each operation in the model, we call the appropriate layer on an input tensor `x`. This function performs the operations inside a U-Net, including the skipped connections with concatenation along the channels dimension. This is the sequence of operations to get input data to a prediction, but it is not the whole model, since that one must predict a sequence of 2 (`integration_steps = 2`). That will be next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843931ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet2(x):\n",
    "    x0 = cube_padding_1(x)\n",
    "    x0 = relu(conv_2d_1(x0))\n",
    "    x0 = cube_padding_1(x0)\n",
    "    x0 = relu(conv_2d_1_2(x0))\n",
    "    x1 = pooling_2(x0)\n",
    "    x1 = cube_padding_1(x1)\n",
    "    x1 = relu(conv_2d_2(x1))\n",
    "    x1 = cube_padding_1(x1)\n",
    "    x1 = relu(conv_2d_2_2(x1))\n",
    "    x2 = pooling_2(x1)\n",
    "    x2 = cube_padding_1(x2)\n",
    "    x2 = relu(conv_2d_5_2(x2))\n",
    "    x2 = cube_padding_1(x2)\n",
    "    x2 = relu(conv_2d_5(x2))\n",
    "    x2 = up_sampling_2(x2)\n",
    "    x = concatenate([x2, x1], axis=-1)\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_6_2(x))\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_6(x))\n",
    "    x = up_sampling_2(x)\n",
    "    x = concatenate([x, x0], axis=-1)\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_7(x))\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_7_2(x))\n",
    "    x = conv_2d_8(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651d4a7",
   "metadata": {},
   "source": [
    "Next we manipulate the result of the CNN back to inputs to the same CNN, add the new insolation input, and pass it through again. This allows us to minimize the loss function at each step of the sequence. Adding the insolation looks complicated because the array includes a time dimension whereas the data inputs are flattened time/variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c173f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Concatenate, Permute\n",
    "\n",
    "def complete_model(x_in):\n",
    "    outputs = []\n",
    "    model_function = globals()[cnn_model_name]\n",
    "    is_seq = isinstance(x_in, (list, tuple))\n",
    "    xi = x_in[0] if is_seq else x_in\n",
    "    outputs.append(model_function(xi))\n",
    "    for step in range(1, integration_steps):\n",
    "        xo = outputs[step - 1]\n",
    "        if is_seq and input_solar:\n",
    "            xo = Reshape(cs[:-1] + (io_time_steps, -1))(xo)\n",
    "            xo = Concatenate(axis=-1)([xo, Permute((2, 3, 4, 1, 5))(x_in[step])])\n",
    "            xo = Reshape(cs)(xo)\n",
    "        outputs.append(model_function(xo))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db57256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "if not input_solar:\n",
    "    inputs = main_input\n",
    "else:\n",
    "    inputs = [main_input]\n",
    "    if input_solar:\n",
    "        inputs = inputs + solar_inputs\n",
    "model = Model(inputs=inputs, outputs=complete_model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "loss_function = 'mse'\n",
    "\n",
    "# Get an optimizer, with mixed precision if requested\n",
    "opt = Adam()\n",
    "if use_mp_optimizer:\n",
    "    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlwp.build_model(model, loss=loss_function, loss_weights=loss_by_step, optimizer=opt, metrics=['mae'])\n",
    "print(dlwp.base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import History, TensorBoard\n",
    "from DLWP.custom import EarlyStoppingMin, SaveWeightsOnEpoch, GeneratorEpochEnd\n",
    "\n",
    "history = History()\n",
    "early = EarlyStoppingMin(monitor='val_loss' if validation_data is not None else 'loss', min_delta=0.,\n",
    "                         min_epochs=min_epochs, max_epochs=max_epochs, patience=patience,\n",
    "                         restore_best_weights=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=log_directory, update_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7468ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "dlwp.fit_generator(tf_train_data, epochs=max_epochs + 1,\n",
    "                   verbose=1, validation_data=tf_val_data,\n",
    "                   callbacks=[history, early, GeneratorEpochEnd(generator)])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.util import save_model\n",
    "\n",
    "save_model(dlwp, model_file, history=history)\n",
    "print('Wrote model %s' % model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e74ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTrain time -- %s seconds --\" % (end_time - start_time))\n",
    "\n",
    "score = dlwp.evaluate(*val_generator.generate([]), verbose=0)\n",
    "print('Validation loss:', score[0])\n",
    "print('Other scores:', score[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55346277",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829f231",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Some user-specified parameters. The `scale_file` contains the mean and standard of the data (which was dropped in the cubed sphere remapping). The `map_files` were produced by the cubed sphere remapping. We can re-use them here so we don't have to generate them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d89e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.pardir)\n",
    "\n",
    "root_directory = '/lcrc/project/AIEADA-2/cubed_sphere_implementation/'\n",
    "predictor_file = os.path.join(root_directory, 'Data', 'tutorial_z500_t2m_CS.nc')\n",
    "scale_file = os.path.join(root_directory, 'Data', 'tutorial_z500_t2m.nc')\n",
    "\n",
    "model = os.path.join(root_directory, 'dlwp-cs_tutorial')\n",
    "map_files = ('map_LL91x180_CS48.nc', 'map_CS48_LL91x180.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "We'll resurrect some parameters from the training tutorial. See that notebook for definitions. Note that we omit `data_interval` because we simply select only every 6 hours from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9871e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_selection = {'varlev': ['z/500', 't2m/0']}\n",
    "add_solar = True\n",
    "io_time_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "validation_set = pd.date_range('2016-12-31', '2018-12-31', freq='6H')\n",
    "validation_set = np.array(validation_set, dtype='datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2017-01-01', '2018-12-31', freq='7D')\n",
    "initialization_dates = xr.DataArray(dates)\n",
    "num_forecast_hours = 5 * 24\n",
    "dt = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.util import load_model, remove_chars, is_channels_last\n",
    "\n",
    "dlwp = load_model(model)\n",
    "\n",
    "# File to save the forecast\n",
    "forecast_file = os.path.join(root_directory, 'forecast_%s.nc' % remove_chars(model.split(os.sep)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = xr.open_dataset(predictor_file)\n",
    "predictor_ds = all_ds.sel(sample=validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import SeriesDataGenerator\n",
    "\n",
    "sequence = dlwp._n_steps if hasattr(dlwp, '_n_steps') and dlwp._n_steps > 1 else None\n",
    "val_generator = SeriesDataGenerator(dlwp, predictor_ds, rank=3, add_insolation=add_solar,\n",
    "                                    input_sel=io_selection, output_sel=io_selection,\n",
    "                                    input_time_steps=io_time_steps, output_time_steps=io_time_steps,\n",
    "                                    shuffle=False, sequence=sequence, batch_size=32,\n",
    "                                    load=False, channels_last=is_channels_last(dlwp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import TimeSeriesEstimator\n",
    "\n",
    "estimator = TimeSeriesEstimator(dlwp, val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b879ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting with model %s...' % model)\n",
    "\n",
    "# Select the samples from the initialization dates. The first \"time\" input to the model is actually one time step earlier\n",
    "samples = np.array([int(np.where(val_generator.ds['sample'] == s)[0]) for s in initialization_dates]) \\\n",
    "    - io_time_steps + 1\n",
    "time_series = estimator.predict(num_forecast_hours // dt, samples=samples, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2493ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose if channels_last was used for the model\n",
    "if is_channels_last(dlwp):\n",
    "    time_series = time_series.transpose('f_hour', 'time', 'varlev', 'x0', 'x1', 'x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale_file is None:\n",
    "    scale_ds = predictor_ds\n",
    "else:\n",
    "    scale_ds = xr.open_dataset(scale_file)\n",
    "sel_mean = scale_ds['mean'].sel(io_selection)\n",
    "sel_std = scale_ds['std'].sel(io_selection)\n",
    "time_series = time_series * sel_std + sel_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.verify import add_metadata_to_forecast_cs\n",
    "\n",
    "fh = np.arange(dt, time_series.shape[0] * dt + 1., dt)\n",
    "time_series = add_metadata_to_forecast_cs(\n",
    "    time_series.values,\n",
    "    fh,\n",
    "    predictor_ds.sel(**io_selection).sel(sample=initialization_dates)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68065a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.drop('varlev').to_netcdf(forecast_file + '.cs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.remap import CubeSphereRemap\n",
    "\n",
    "csr = CubeSphereRemap(to_netcdf4=True)\n",
    "csr.assign_maps(*map_files)\n",
    "csr.convert_from_faces(forecast_file + '.cs', forecast_file + '.tmp')\n",
    "csr.inverse_remap(forecast_file + '.tmp', forecast_file, '--var', 'forecast')\n",
    "os.remove(forecast_file + '.tmp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
